{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2b60b7-a838-497e-a0eb-4457d4601079",
   "metadata": {},
   "source": [
    "# COMMON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27a72d7-8146-4abc-a896-da06b36f520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 17:57:27.212460: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-15 17:57:27.212696: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-15 17:57:27.242803: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-15 17:57:27.319515: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-15 17:57:42.855072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jiwer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/tmp/ipykernel_61437/1074209537.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjiwer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wer,cer\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_login\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jiwer'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jiwer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m00_common.ipynb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2432\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2430\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2432\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2434\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2436\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/IPython/core/magics/execution.py:737\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 737\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_execfile_ipy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2954\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[0;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[1;32m   2952\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[0;32m-> 2954\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m   2956\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:270\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/tmp/ipykernel_61437/1074209537.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjiwer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wer,cer\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_login\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jiwer'"
     ]
    }
   ],
   "source": [
    "%run 00_common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad08702-415c-4aaf-a735-1c6969897972",
   "metadata": {},
   "source": [
    "# READ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c15ffa6-5840-4739-b128-c5874d566e7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torgo_xlsr_finetune_F01\n",
      "torgo_xlsr_finetune_F01_keep_all\n",
      "torgo_xlsr_finetune_F03\n",
      "torgo_xlsr_finetune_F03_keep_all\n",
      "torgo_xlsr_finetune_F04\n",
      "torgo_xlsr_finetune_F04_keep_all\n",
      "torgo_xlsr_finetune_M01\n",
      "torgo_xlsr_finetune_M01_keep_all\n",
      "torgo_xlsr_finetune_M02\n",
      "torgo_xlsr_finetune_M02_keep_all\n",
      "torgo_xlsr_finetune_M03\n",
      "torgo_xlsr_finetune_M03_keep_all\n",
      "torgo_xlsr_finetune_M04\n",
      "torgo_xlsr_finetune_M04_keep_all\n",
      "torgo_xlsr_finetune_M05\n",
      "torgo_xlsr_finetune_M05_keep_all\n"
     ]
    }
   ],
   "source": [
    "dataframes = {}\n",
    "\n",
    "def load_dataframes(directory='results'):\n",
    "    dataframes = {}\n",
    "\n",
    "    for speaker_dir in os.listdir(directory):\n",
    "        if os.path.isdir(os.path.join(directory, speaker_dir)) and not speaker_dir.endswith('.ipynb_checkpoints'):\n",
    "            print(speaker_dir)\n",
    "            train_df = None\n",
    "            test_df = None\n",
    "            valid_df = None\n",
    "\n",
    "            for file in os.listdir(os.path.join(directory, speaker_dir)):\n",
    "                if file.endswith('.csv'):\n",
    "                    df_name = os.path.splitext(file)[0]\n",
    "                    df_path = os.path.join(directory, speaker_dir, file)\n",
    "\n",
    "                    if 'train' in df_name:\n",
    "                        if train_df is None:\n",
    "                            train_df = pd.read_csv(df_path)\n",
    "                        else:\n",
    "                            train_df = pd.concat([train_df, pd.read_csv(df_path)], ignore_index=True)\n",
    "                    elif 'test' in df_name:\n",
    "                        if test_df is None:\n",
    "                            test_df = pd.read_csv(df_path)\n",
    "                        else:\n",
    "                            test_df = pd.concat([test_df, pd.read_csv(df_path)], ignore_index=True)\n",
    "                    elif 'validation' in df_name:\n",
    "                        if valid_df is None:\n",
    "                            valid_df = pd.read_csv(df_path)\n",
    "                        else:\n",
    "                            valid_df = pd.concat([valid_df, pd.read_csv(df_path)], ignore_index=True)\n",
    "\n",
    "            combined_df = pd.concat([train_df, test_df, valid_df], ignore_index=True)\n",
    "\n",
    "            if combined_df is not None:\n",
    "                dataframes[speaker_dir] = combined_df\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "dataframes = load_dataframes()\n",
    "\n",
    "\n",
    "# for df_name, df in dataframes.items():\n",
    "#     print(f'DataFrame: {df_name}')\n",
    "#     print(df)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73232ce-b96a-43f3-9e77-696d0b83f409",
   "metadata": {},
   "source": [
    "# SEPERATE CSV BY IF KEEP ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5602dd43-67ab-4f5b-8395-29c08296aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_keep_all = {}\n",
    "dataframes_no_keep_all = {}\n",
    "\n",
    "for speaker_dir in os.listdir('results'):\n",
    "    if os.path.isdir(os.path.join('results', speaker_dir)) and not speaker_dir.endswith('.ipynb_checkpoints'):\n",
    "        # speaker_dir = speaker_dir.rstrip('_')\n",
    "        train_df = None\n",
    "        test_df = None\n",
    "        valid_df = None\n",
    "\n",
    "        for file in os.listdir(os.path.join('results', speaker_dir)):\n",
    "            if file.endswith('.csv'):\n",
    "                df_name = os.path.splitext(file)[0]\n",
    "                df_path = os.path.join('results', speaker_dir, file)\n",
    "                \n",
    "                if 'train' in df_name:\n",
    "                    if train_df is None:\n",
    "                        train_df = pd.read_csv(df_path)\n",
    "                    else:\n",
    "                        train_df = pd.concat([train_df, pd.read_csv(df_path)], ignore_index=True)\n",
    "                elif 'test' in df_name:\n",
    "                    if test_df is None:\n",
    "                        test_df = pd.read_csv(df_path)\n",
    "                    else:\n",
    "                        test_df = pd.concat([test_df, pd.read_csv(df_path)], ignore_index=True)\n",
    "                elif 'validation' in df_name:\n",
    "                    if valid_df is None:\n",
    "                        valid_df = pd.read_csv(df_path)\n",
    "                    else:\n",
    "                        valid_df = pd.concat([valid_df, pd.read_csv(df_path)], ignore_index=True)\n",
    "        \n",
    "        combined_df = pd.concat([train_df, test_df, valid_df], ignore_index=True)\n",
    "        \n",
    "        if 'keep_all' in speaker_dir:\n",
    "            dataframes_keep_all[speaker_dir] = combined_df\n",
    "        else:\n",
    "            dataframes_no_keep_all[speaker_dir] = combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81dd0534-5855-4ca4-bb44-54acf80fd56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torgo_xlsr_finetune_F01 - Data Size: 14409\n",
      "torgo_xlsr_finetune_F03 - Data Size: 8568\n",
      "torgo_xlsr_finetune_F04 - Data Size: 9454\n",
      "torgo_xlsr_finetune_M01 - Data Size: 6540\n",
      "torgo_xlsr_finetune_M02 - Data Size: 9790\n",
      "torgo_xlsr_finetune_M03 - Data Size: 9572\n",
      "torgo_xlsr_finetune_M04 - Data Size: 11751\n",
      "torgo_xlsr_finetune_M05 - Data Size: 8853\n",
      "\n",
      "torgo_xlsr_finetune_F01_keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_F03_keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_F04_keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_M01_keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_M02_keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_M03_keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_M04_keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_M05_keep_all - Data Size: 16082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_dataframe_sizes(dataframes_no_keep_all)\n",
    "print_dataframe_sizes(dataframes_keep_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc909ece-2f4e-408d-b0dd-d00a180a82f5",
   "metadata": {},
   "source": [
    "# SEPERATE SENTENCE AND WORD LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a47aca0-92a5-4122-85dd-61fdc084e2e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96966/2810713955.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torgo_xlsr_finetune_F01_keep_all - Data Size: 12129\n",
      "torgo_xlsr_finetune_F03_keep_all - Data Size: 12129\n",
      "torgo_xlsr_finetune_F04_keep_all - Data Size: 12129\n",
      "torgo_xlsr_finetune_M01_keep_all - Data Size: 12115\n",
      "torgo_xlsr_finetune_M02_keep_all - Data Size: 12129\n",
      "torgo_xlsr_finetune_M03_keep_all - Data Size: 12129\n",
      "torgo_xlsr_finetune_M04_keep_all - Data Size: 12129\n",
      "torgo_xlsr_finetune_M05_keep_all - Data Size: 12129\n",
      "\n",
      "torgo_xlsr_finetune_F01_keep_all - Data Size: 3953\n",
      "torgo_xlsr_finetune_F03_keep_all - Data Size: 3953\n",
      "torgo_xlsr_finetune_F04_keep_all - Data Size: 3953\n",
      "torgo_xlsr_finetune_M01_keep_all - Data Size: 3967\n",
      "torgo_xlsr_finetune_M02_keep_all - Data Size: 3953\n",
      "torgo_xlsr_finetune_M03_keep_all - Data Size: 3953\n",
      "torgo_xlsr_finetune_M04_keep_all - Data Size: 3953\n",
      "torgo_xlsr_finetune_M05_keep_all - Data Size: 3953\n",
      "\n",
      "torgo_xlsr_finetune_F01 - Data Size: 10795\n",
      "torgo_xlsr_finetune_F03 - Data Size: 6813\n",
      "torgo_xlsr_finetune_F04 - Data Size: 7482\n",
      "torgo_xlsr_finetune_M01 - Data Size: 4404\n",
      "torgo_xlsr_finetune_M02 - Data Size: 7687\n",
      "torgo_xlsr_finetune_M03 - Data Size: 7480\n",
      "torgo_xlsr_finetune_M04 - Data Size: 9139\n",
      "torgo_xlsr_finetune_M05 - Data Size: 7178\n",
      "\n",
      "torgo_xlsr_finetune_F01 - Data Size: 3614\n",
      "torgo_xlsr_finetune_F03 - Data Size: 1755\n",
      "torgo_xlsr_finetune_F04 - Data Size: 1972\n",
      "torgo_xlsr_finetune_M01 - Data Size: 2136\n",
      "torgo_xlsr_finetune_M02 - Data Size: 2103\n",
      "torgo_xlsr_finetune_M03 - Data Size: 2092\n",
      "torgo_xlsr_finetune_M04 - Data Size: 2612\n",
      "torgo_xlsr_finetune_M05 - Data Size: 1675\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96966/2810713955.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_96966/2810713955.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dataframes_word_keep_all = {}\n",
    "dataframes_sentence_keep_all = {}\n",
    "\n",
    "for speaker_dir, combined_df in dataframes_keep_all.items():\n",
    "    combined_df['word_count'] = combined_df['references'].str.split().apply(len)\n",
    "    word_df = combined_df[combined_df['word_count'] == 1]\n",
    "    sentence_df = combined_df[combined_df['word_count'] > 1]\n",
    "    word_df.drop(columns=['word_count'], inplace=True)\n",
    "    sentence_df.drop(columns=['word_count'], inplace=True)\n",
    "    dataframes_word_keep_all[speaker_dir] = word_df\n",
    "    dataframes_sentence_keep_all[speaker_dir] = sentence_df\n",
    "\n",
    "dataframes_word_no_keep_all = {}\n",
    "dataframes_sentence_no_keep_all = {}\n",
    "\n",
    "for speaker_dir, combined_df in dataframes_no_keep_all.items():\n",
    "    combined_df['word_count'] = combined_df['references'].str.split().apply(len)\n",
    "    word_df = combined_df[combined_df['word_count'] == 1]\n",
    "    sentence_df = combined_df[combined_df['word_count'] > 1]\n",
    "    word_df.drop(columns=['word_count'], inplace=True)\n",
    "    sentence_df.drop(columns=['word_count'], inplace=True)\n",
    "    dataframes_word_no_keep_all[speaker_dir] = word_df\n",
    "    dataframes_sentence_no_keep_all[speaker_dir] = sentence_df\n",
    "\n",
    "print_dataframe_sizes(dataframes_word_keep_all)\n",
    "print_dataframe_sizes(dataframes_sentence_keep_all)\n",
    "print_dataframe_sizes(dataframes_word_no_keep_all)\n",
    "print_dataframe_sizes(dataframes_sentence_no_keep_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bbe60-fc72-4f5f-82a6-ed97117873a2",
   "metadata": {},
   "source": [
    "# REMOVE SAME COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea7c23da-d20c-485a-abf0-718fe5ecf70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframes(dataframes_dict, col_names):\n",
    "    processed_dataframes = {}\n",
    "\n",
    "    for df_name, df in dataframes_dict.items():\n",
    "        # Remove duplicates and drop NaN values\n",
    "        df = df.drop_duplicates(subset=col_names, keep=False)\n",
    "        df = df.dropna() \n",
    "        processed_dataframes[df_name] = df\n",
    "\n",
    "    return processed_dataframes\n",
    "\n",
    "def convert_to_phonemes(data_frame, remove_num=True):\n",
    "    g2p = G2p()\n",
    "\n",
    "    for df_name, df in data_frame.items():\n",
    "        df['predictions_phoneme'] = df['predictions'].apply(lambda x: \" \".join(g2p(x)))\n",
    "        df['references_phoneme'] = df['references'].apply(lambda x: \" \".join(g2p(x)))\n",
    "\n",
    "        if remove_num:\n",
    "            # Remove numeric values\n",
    "            df['predictions_phoneme'] = df['predictions_phoneme'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "            df['references_phoneme'] = df['references_phoneme'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "        df.drop(['predictions', 'references'], axis=1, inplace=True)\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "def split_dataframes(dataframes_word):\n",
    "    # Remove Duplicates\n",
    "    for df_name, df in dataframes_word.items():\n",
    "        # Remove rows where columns are the same\n",
    "        df = df[df['predictions_phoneme'] != df['references_phoneme']]\n",
    "        \n",
    "        # Remove rows where columns only differ by the number of spaces\n",
    "        df = df[df.apply(lambda row: row['predictions_phoneme'].strip() != row['references_phoneme'].strip(), axis=1)]\n",
    "        \n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    ori_train_dataframes_word = {}\n",
    "    test_dataframes_word = {}\n",
    "    \n",
    "    for df_name, df in dataframes_word.items():\n",
    "        # Train-Test Split\n",
    "        df = df.drop_duplicates(subset=['predictions_phoneme', 'references_phoneme'], keep=False)\n",
    "        train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "        \n",
    "        # Naming variables without pattern\n",
    "        train_df_name = df_name\n",
    "        test_df_name = f\"{df_name}_test\"\n",
    "        \n",
    "        ori_train_dataframes_word[train_df_name] = train_df\n",
    "        test_dataframes_word[test_df_name] = test_df\n",
    "    \n",
    "    train_dataframes_word = {}\n",
    "    val_dataframes_word = {} \n",
    "    \n",
    "    for df_name, df in ori_train_dataframes_word.items():\n",
    "        df = df.drop_duplicates(subset=['predictions_phoneme', 'references_phoneme'], keep=False)\n",
    "        train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "        train_df_name = f\"{df_name}_train\"\n",
    "        val_df_name = f\"{df_name}_val\"\n",
    "        \n",
    "        train_dataframes_word[train_df_name] = train_df\n",
    "        val_dataframes_word[val_df_name] = val_df\n",
    "    print_dataframe_sizes(train_dataframes_word)\n",
    "    print_dataframe_sizes(val_dataframes_word)\n",
    "    print_dataframe_sizes(test_dataframes_word)\n",
    " \n",
    "    return train_dataframes_word, val_dataframes_word, test_dataframes_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4edbb86-4ac4-4588-a085-519b3b9b6fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torgo_xlsr_finetune_F01_keep_all - Data Size: 336\n",
      "torgo_xlsr_finetune_F03_keep_all - Data Size: 328\n",
      "torgo_xlsr_finetune_F04_keep_all - Data Size: 296\n",
      "torgo_xlsr_finetune_M01_keep_all - Data Size: 399\n",
      "torgo_xlsr_finetune_M02_keep_all - Data Size: 534\n",
      "torgo_xlsr_finetune_M03_keep_all - Data Size: 288\n",
      "torgo_xlsr_finetune_M04_keep_all - Data Size: 449\n",
      "torgo_xlsr_finetune_M05_keep_all - Data Size: 452\n",
      "\n",
      "torgo_xlsr_finetune_F01_keep_all - Data Size: 239\n",
      "torgo_xlsr_finetune_F03_keep_all - Data Size: 229\n",
      "torgo_xlsr_finetune_F04_keep_all - Data Size: 236\n",
      "torgo_xlsr_finetune_M01_keep_all - Data Size: 336\n",
      "torgo_xlsr_finetune_M02_keep_all - Data Size: 355\n",
      "torgo_xlsr_finetune_M03_keep_all - Data Size: 223\n",
      "torgo_xlsr_finetune_M04_keep_all - Data Size: 270\n",
      "torgo_xlsr_finetune_M05_keep_all - Data Size: 273\n",
      "\n",
      "torgo_xlsr_finetune_F01 - Data Size: 305\n",
      "torgo_xlsr_finetune_F03 - Data Size: 356\n",
      "torgo_xlsr_finetune_F04 - Data Size: 340\n",
      "torgo_xlsr_finetune_M01 - Data Size: 352\n",
      "torgo_xlsr_finetune_M02 - Data Size: 405\n",
      "torgo_xlsr_finetune_M03 - Data Size: 285\n",
      "torgo_xlsr_finetune_M04 - Data Size: 432\n",
      "torgo_xlsr_finetune_M05 - Data Size: 404\n",
      "\n",
      "torgo_xlsr_finetune_F01 - Data Size: 217\n",
      "torgo_xlsr_finetune_F03 - Data Size: 317\n",
      "torgo_xlsr_finetune_F04 - Data Size: 243\n",
      "torgo_xlsr_finetune_M01 - Data Size: 239\n",
      "torgo_xlsr_finetune_M02 - Data Size: 256\n",
      "torgo_xlsr_finetune_M03 - Data Size: 242\n",
      "torgo_xlsr_finetune_M04 - Data Size: 223\n",
      "torgo_xlsr_finetune_M05 - Data Size: 149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframes_word_keep_all = process_dataframes(dataframes_word_keep_all, col_names)\n",
    "dataframes_sentence_keep_all = process_dataframes(dataframes_sentence_keep_all, col_names)\n",
    "dataframes_word_no_keep_all = process_dataframes(dataframes_word_no_keep_all, col_names)\n",
    "dataframes_sentence_no_keep_all = process_dataframes(dataframes_sentence_no_keep_all, col_names)\n",
    "\n",
    "print_dataframe_sizes(dataframes_word_keep_all)\n",
    "print_dataframe_sizes(dataframes_sentence_keep_all)\n",
    "print_dataframe_sizes(dataframes_word_no_keep_all)\n",
    "print_dataframe_sizes(dataframes_sentence_no_keep_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c3bb03-2f66-4334-a0d3-fb0b56b0c9b4",
   "metadata": {},
   "source": [
    "# output dont Keep ALL phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cbeeb55-2da6-4137-a085-be6e40333bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_word_no_keep_all = convert_to_phonemes(dataframes_word_no_keep_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edce4394-160f-4fe2-8548-7e9d93d96fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torgo_xlsr_finetune_F01_train - Data Size: 228\n",
      "torgo_xlsr_finetune_F03_train - Data Size: 264\n",
      "torgo_xlsr_finetune_F04_train - Data Size: 256\n",
      "torgo_xlsr_finetune_M01_train - Data Size: 271\n",
      "torgo_xlsr_finetune_M02_train - Data Size: 318\n",
      "torgo_xlsr_finetune_M03_train - Data Size: 213\n",
      "torgo_xlsr_finetune_M04_train - Data Size: 333\n",
      "torgo_xlsr_finetune_M05_train - Data Size: 311\n",
      "\n",
      "torgo_xlsr_finetune_F01_val - Data Size: 26\n",
      "torgo_xlsr_finetune_F03_val - Data Size: 30\n",
      "torgo_xlsr_finetune_F04_val - Data Size: 29\n",
      "torgo_xlsr_finetune_M01_val - Data Size: 31\n",
      "torgo_xlsr_finetune_M02_val - Data Size: 36\n",
      "torgo_xlsr_finetune_M03_val - Data Size: 24\n",
      "torgo_xlsr_finetune_M04_val - Data Size: 37\n",
      "torgo_xlsr_finetune_M05_val - Data Size: 35\n",
      "\n",
      "torgo_xlsr_finetune_F01_test - Data Size: 29\n",
      "torgo_xlsr_finetune_F03_test - Data Size: 33\n",
      "torgo_xlsr_finetune_F04_test - Data Size: 32\n",
      "torgo_xlsr_finetune_M01_test - Data Size: 34\n",
      "torgo_xlsr_finetune_M02_test - Data Size: 40\n",
      "torgo_xlsr_finetune_M03_test - Data Size: 27\n",
      "torgo_xlsr_finetune_M04_test - Data Size: 42\n",
      "torgo_xlsr_finetune_M05_test - Data Size: 39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataframes_word, val_dataframes_word, test_dataframes_word=split_dataframes(dataframes_word_no_keep_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e09ed0e5-ef31-40f9-9904-916645ba6609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker: torgo_xlsr_finetune_F01_test\n",
      "Word Error Rate (WER): 68.32%\n",
      "Character Error Rate (CER): 55.19%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_F03_test\n",
      "Word Error Rate (WER): 51.64%\n",
      "Character Error Rate (CER): 39.15%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_F04_test\n",
      "Word Error Rate (WER): 40.54%\n",
      "Character Error Rate (CER): 31.38%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_M01_test\n",
      "Word Error Rate (WER): 55.64%\n",
      "Character Error Rate (CER): 43.25%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_M02_test\n",
      "Word Error Rate (WER): 68.92%\n",
      "Character Error Rate (CER): 54.78%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_M03_test\n",
      "Word Error Rate (WER): 51.55%\n",
      "Character Error Rate (CER): 42.79%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_M04_test\n",
      "Word Error Rate (WER): 59.24%\n",
      "Character Error Rate (CER): 46.85%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_M05_test\n",
      "Word Error Rate (WER): 53.62%\n",
      "Character Error Rate (CER): 40.55%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_error_rates(dataframes, phoneme_names=['predictions', 'references']):\n",
    "    for df_name, df in dataframes.items():\n",
    "        references = df[phoneme_names[1]].tolist()\n",
    "        hypotheses = df[phoneme_names[0]].tolist()\n",
    "\n",
    "        wer_score = wer(references, hypotheses)\n",
    "        cer_score = cer(references, hypotheses)\n",
    "\n",
    "        print(f'Speaker: {df_name}')\n",
    "        print(f'Word Error Rate (WER): {wer_score:.2%}')\n",
    "        print(f'Character Error Rate (CER): {cer_score:.2%}')\n",
    "        print()\n",
    "\n",
    "calculate_error_rates(test_dataframes_word, phoneme_names=['predictions_phoneme', 'references_phoneme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3aa313bf-adf5-4468-9e6d-13ccdb718de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframes_to_csv(train_dataframes, val_dataframes, test_dataframes, torgo_train_type):\n",
    "    output_folder = f'data/{torgo_train_type}'\n",
    "    # print(f'df_name:{df_name}')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for df_name, train_df in train_dataframes.items():\n",
    "        train_df.to_csv(os.path.join(output_folder, f'{df_name}.csv'), index=False)\n",
    "\n",
    "    for df_name, val_df in val_dataframes.items():\n",
    "        val_df.to_csv(os.path.join(output_folder, f'{df_name}.csv'), index=False)\n",
    "\n",
    "    for df_name, test_df in test_dataframes.items():\n",
    "        test_df.to_csv(os.path.join(output_folder, f'{df_name}.csv'), index=False)\n",
    "\n",
    "\n",
    "def convert_csv_to_json(csv_path, json_output_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    json_data = df.to_json(orient='records')\n",
    "\n",
    "    with open(json_output_path, 'w') as json_file:\n",
    "        json_file.write(json_data)\n",
    "\n",
    "TORGO_TRAIN_TYPE=TorgoTrainType.WORD_NO_KEEP.value\n",
    "save_dataframes_to_csv(train_dataframes_word, \n",
    "                       val_dataframes_word, \n",
    "                       test_dataframes_word, \n",
    "                       TORGO_TRAIN_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14a37034-2841-4bc8-a553-d655bfbfcaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_folder = f'data/{TORGO_TRAIN_TYPE}'\n",
    "json_folder = f'data/{TORGO_TRAIN_TYPE}'\n",
    "\n",
    "os.makedirs(json_folder, exist_ok=True)\n",
    "\n",
    "for csv_file_name in os.listdir(csv_folder):\n",
    "    if csv_file_name.endswith('.csv'):\n",
    "        csv_file_path = os.path.join(csv_folder, csv_file_name)\n",
    "        json_file_name = csv_file_name.replace('.csv', '.json')\n",
    "        json_output_file_path = os.path.join(json_folder, json_file_name)\n",
    "\n",
    "        convert_csv_to_json(csv_file_path, json_output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a9adf-5030-4b92-9b9a-a4815bde7e2a",
   "metadata": {},
   "source": [
    "# output Keep ALL phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9ff8720-ef11-4b7b-bea1-ecfe3436fe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torgo_xlsr_finetune_F01_keep_all_train - Data Size: 257\n",
      "torgo_xlsr_finetune_F03_keep_all_train - Data Size: 251\n",
      "torgo_xlsr_finetune_F04_keep_all_train - Data Size: 225\n",
      "torgo_xlsr_finetune_M01_keep_all_train - Data Size: 299\n",
      "torgo_xlsr_finetune_M02_keep_all_train - Data Size: 420\n",
      "torgo_xlsr_finetune_M03_keep_all_train - Data Size: 226\n",
      "torgo_xlsr_finetune_M04_keep_all_train - Data Size: 340\n",
      "torgo_xlsr_finetune_M05_keep_all_train - Data Size: 348\n",
      "\n",
      "torgo_xlsr_finetune_F01_keep_all_val - Data Size: 29\n",
      "torgo_xlsr_finetune_F03_keep_all_val - Data Size: 28\n",
      "torgo_xlsr_finetune_F04_keep_all_val - Data Size: 26\n",
      "torgo_xlsr_finetune_M01_keep_all_val - Data Size: 34\n",
      "torgo_xlsr_finetune_M02_keep_all_val - Data Size: 47\n",
      "torgo_xlsr_finetune_M03_keep_all_val - Data Size: 26\n",
      "torgo_xlsr_finetune_M04_keep_all_val - Data Size: 38\n",
      "torgo_xlsr_finetune_M05_keep_all_val - Data Size: 39\n",
      "\n",
      "torgo_xlsr_finetune_F01_keep_all_test - Data Size: 32\n",
      "torgo_xlsr_finetune_F03_keep_all_test - Data Size: 32\n",
      "torgo_xlsr_finetune_F04_keep_all_test - Data Size: 28\n",
      "torgo_xlsr_finetune_M01_keep_all_test - Data Size: 37\n",
      "torgo_xlsr_finetune_M02_keep_all_test - Data Size: 52\n",
      "torgo_xlsr_finetune_M03_keep_all_test - Data Size: 28\n",
      "torgo_xlsr_finetune_M04_keep_all_test - Data Size: 42\n",
      "torgo_xlsr_finetune_M05_keep_all_test - Data Size: 43\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_F01_keep_all_test\n",
      "Word Error Rate (WER): 63.37%\n",
      "Character Error Rate (CER): 45.97%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_F03_keep_all_test\n",
      "Word Error Rate (WER): 63.06%\n",
      "Character Error Rate (CER): 50.00%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_F04_keep_all_test\n",
      "Word Error Rate (WER): 68.13%\n",
      "Character Error Rate (CER): 53.68%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_M01_keep_all_test\n",
      "Word Error Rate (WER): 70.16%\n",
      "Character Error Rate (CER): 54.75%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_M02_keep_all_test\n",
      "Word Error Rate (WER): 66.67%\n",
      "Character Error Rate (CER): 49.28%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_M03_keep_all_test\n",
      "Word Error Rate (WER): 64.52%\n",
      "Character Error Rate (CER): 50.77%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_M04_keep_all_test\n",
      "Word Error Rate (WER): 65.62%\n",
      "Character Error Rate (CER): 51.16%\n",
      "\n",
      "Speaker: torgo_xlsr_finetune_M05_keep_all_test\n",
      "Word Error Rate (WER): 61.59%\n",
      "Character Error Rate (CER): 53.89%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TORGO_TRAIN_TYPE=TorgoTrainType.WORD_KEEP.value\n",
    "dataframes_word_keep_all = convert_to_phonemes(dataframes_word_keep_all)\n",
    "train_dataframes_word, val_dataframes_word, test_dataframes_word=split_dataframes(dataframes_word_keep_all)\n",
    "calculate_error_rates(test_dataframes_word, phoneme_names=['predictions_phoneme', 'references_phoneme'])\n",
    "save_dataframes_to_csv(train_dataframes_word, \n",
    "                       val_dataframes_word, \n",
    "                       test_dataframes_word, \n",
    "                       TORGO_TRAIN_TYPE)\n",
    "\n",
    "csv_folder = f'data/{TORGO_TRAIN_TYPE}'\n",
    "json_folder = f'data/{TORGO_TRAIN_TYPE}'\n",
    "os.makedirs(json_folder, exist_ok=True)\n",
    "\n",
    "for csv_file_name in os.listdir(csv_folder):\n",
    "    if csv_file_name.endswith('.csv'):\n",
    "        csv_file_path = os.path.join(csv_folder, csv_file_name)\n",
    "        json_file_name = csv_file_name.replace('.csv', '.json')\n",
    "        json_output_file_path = os.path.join(json_folder, json_file_name)\n",
    "\n",
    "        convert_csv_to_json(csv_file_path, json_output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7304e8-644d-4b9b-82bb-163b14e7e620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
