{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf8bba9-daa2-470c-8767-1d799b175d2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions_phoneme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27.0\n",
      "torgo_xlsr_finetune_F01 - Data Size: 14409\n",
      "torgo_xlsr_finetune_F03 - Data Size: 8568\n",
      "torgo_xlsr_finetune_F04 - Data Size: 9454\n",
      "torgo_xlsr_finetune_M01 - Data Size: 6540\n",
      "torgo_xlsr_finetune_M02 - Data Size: 9790\n",
      "torgo_xlsr_finetune_M03 - Data Size: 9572\n",
      "torgo_xlsr_finetune_M04 - Data Size: 11751\n",
      "torgo_xlsr_finetune_M05 - Data Size: 8853\n",
      "torgo_xlsr_finetune_F01_keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_F03_keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_F04_keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_M01_keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_M02_keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_M03_keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_M04__keep_all - Data Size: 16082\n",
      "torgo_xlsr_finetune_M05_keep_all - Data Size: 16082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21349/2226408003.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_df.drop(columns=['word_count'], inplace=True)\n",
      "/tmp/ipykernel_21349/2226408003.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentence_df.drop(columns=['word_count'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torgo_xlsr_finetune_F01_keep_all - Data Size: 12129\n",
      "torgo_xlsr_finetune_F03_keep_all - Data Size: 12129\n",
      "torgo_xlsr_finetune_F04_keep_all - Data Size: 12129\n",
      "torgo_xlsr_finetune_M01_keep_all - Data Size: 12115\n",
      "torgo_xlsr_finetune_M02_keep_all - Data Size: 12129\n",
      "torgo_xlsr_finetune_M03_keep_all - Data Size: 12129\n",
      "torgo_xlsr_finetune_M04__keep_all - Data Size: 12129\n",
      "torgo_xlsr_finetune_M05_keep_all - Data Size: 12129\n",
      "\n",
      "torgo_xlsr_finetune_F01_keep_all - Data Size: 3953\n",
      "torgo_xlsr_finetune_F03_keep_all - Data Size: 3953\n",
      "torgo_xlsr_finetune_F04_keep_all - Data Size: 3953\n",
      "torgo_xlsr_finetune_M01_keep_all - Data Size: 3967\n",
      "torgo_xlsr_finetune_M02_keep_all - Data Size: 3953\n",
      "torgo_xlsr_finetune_M03_keep_all - Data Size: 3953\n",
      "torgo_xlsr_finetune_M04__keep_all - Data Size: 3953\n",
      "torgo_xlsr_finetune_M05_keep_all - Data Size: 3953\n",
      "\n",
      "torgo_xlsr_finetune_F01 - Data Size: 10795\n",
      "torgo_xlsr_finetune_F03 - Data Size: 6813\n",
      "torgo_xlsr_finetune_F04 - Data Size: 7482\n",
      "torgo_xlsr_finetune_M01 - Data Size: 4404\n",
      "torgo_xlsr_finetune_M02 - Data Size: 7687\n",
      "torgo_xlsr_finetune_M03 - Data Size: 7480\n",
      "torgo_xlsr_finetune_M04 - Data Size: 9139\n",
      "torgo_xlsr_finetune_M05 - Data Size: 7178\n",
      "\n",
      "torgo_xlsr_finetune_F01 - Data Size: 3614\n",
      "torgo_xlsr_finetune_F03 - Data Size: 1755\n",
      "torgo_xlsr_finetune_F04 - Data Size: 1972\n",
      "torgo_xlsr_finetune_M01 - Data Size: 2136\n",
      "torgo_xlsr_finetune_M02 - Data Size: 2103\n",
      "torgo_xlsr_finetune_M03 - Data Size: 2092\n",
      "torgo_xlsr_finetune_M04 - Data Size: 2612\n",
      "torgo_xlsr_finetune_M05 - Data Size: 1675\n",
      "      predictions references\n",
      "0             fee        fee\n",
      "2            knew       knew\n",
      "3            left       left\n",
      "4             air        air\n",
      "5            torn       torn\n",
      "...           ...        ...\n",
      "16075        must       nest\n",
      "16076       ruggd       ride\n",
      "16077         hem        him\n",
      "16079        sell       sell\n",
      "16080       spark      spark\n",
      "\n",
      "[12129 rows x 2 columns]\n",
      "      predictions references\n",
      "0           stick      stick\n",
      "1             pat        pat\n",
      "2              up         up\n",
      "3            meat       meat\n",
      "4              no       know\n",
      "...           ...        ...\n",
      "16073       witch      witch\n",
      "16076         nut        nut\n",
      "16077        sell       sell\n",
      "16080         sin        sin\n",
      "16081      victor     victor\n",
      "\n",
      "[12129 rows x 2 columns]\n",
      "      predictions references\n",
      "0           stick      stick\n",
      "1             pat        pat\n",
      "2              up         up\n",
      "3            meat       meat\n",
      "4              no       know\n",
      "...           ...        ...\n",
      "16075        nest       nest\n",
      "16076        rige       ride\n",
      "16077         hem        him\n",
      "16079         the       sell\n",
      "16080       spark      spark\n",
      "\n",
      "[12129 rows x 2 columns]\n",
      "      predictions references\n",
      "0           stick      stick\n",
      "1             pat        pat\n",
      "2              up         up\n",
      "3            meat       meat\n",
      "4             now       know\n",
      "...           ...        ...\n",
      "16075        must       nest\n",
      "16076        ride       ride\n",
      "16077         hem        him\n",
      "16079        selw       sell\n",
      "16080       spark      spark\n",
      "\n",
      "[12115 rows x 2 columns]\n",
      "      predictions references\n",
      "0           stick      stick\n",
      "1             pat        pat\n",
      "2              up         up\n",
      "3            meat       meat\n",
      "4              no       know\n",
      "...           ...        ...\n",
      "16075        nest       nest\n",
      "16076       rugge       ride\n",
      "16077         hem        him\n",
      "16079         sew       sell\n",
      "16080       spark      spark\n",
      "\n",
      "[12129 rows x 2 columns]\n",
      "      predictions references\n",
      "0           stick      stick\n",
      "1             pat        pat\n",
      "2              up         up\n",
      "3            meat       meat\n",
      "4              no       know\n",
      "...           ...        ...\n",
      "16075        must       nest\n",
      "16076         rug       ride\n",
      "16077         hem        him\n",
      "16079        sell       sell\n",
      "16080       spark      spark\n",
      "\n",
      "[12129 rows x 2 columns]\n",
      "      predictions references\n",
      "0           stick      stick\n",
      "1             pat        pat\n",
      "2              up         up\n",
      "3            meat       meat\n",
      "4              no       know\n",
      "...           ...        ...\n",
      "16075        must       nest\n",
      "16076         fge       ride\n",
      "16077         hem        him\n",
      "16079         sew       sell\n",
      "16080       spark      spark\n",
      "\n",
      "[12129 rows x 2 columns]\n",
      "      predictions references\n",
      "0           stick      stick\n",
      "1             pat        pat\n",
      "2              up         up\n",
      "3            meat       meat\n",
      "4            know       know\n",
      "...           ...        ...\n",
      "16075        must       nest\n",
      "16076        ride       ride\n",
      "16077         hem        him\n",
      "16079        sell       sell\n",
      "16080       spark      spark\n",
      "\n",
      "[12129 rows x 2 columns]\n",
      "torgo_xlsr_finetune_F01_keep_all - Data Size: 336\n",
      "torgo_xlsr_finetune_F03_keep_all - Data Size: 328\n",
      "torgo_xlsr_finetune_F04_keep_all - Data Size: 296\n",
      "torgo_xlsr_finetune_M01_keep_all - Data Size: 399\n",
      "torgo_xlsr_finetune_M02_keep_all - Data Size: 534\n",
      "torgo_xlsr_finetune_M03_keep_all - Data Size: 288\n",
      "torgo_xlsr_finetune_M04__keep_all - Data Size: 449\n",
      "torgo_xlsr_finetune_M05_keep_all - Data Size: 452\n",
      "\n",
      "torgo_xlsr_finetune_F01_keep_all - Data Size: 239\n",
      "torgo_xlsr_finetune_F03_keep_all - Data Size: 229\n",
      "torgo_xlsr_finetune_F04_keep_all - Data Size: 236\n",
      "torgo_xlsr_finetune_M01_keep_all - Data Size: 336\n",
      "torgo_xlsr_finetune_M02_keep_all - Data Size: 355\n",
      "torgo_xlsr_finetune_M03_keep_all - Data Size: 223\n",
      "torgo_xlsr_finetune_M04__keep_all - Data Size: 270\n",
      "torgo_xlsr_finetune_M05_keep_all - Data Size: 273\n",
      "\n",
      "torgo_xlsr_finetune_F01 - Data Size: 305\n",
      "torgo_xlsr_finetune_F03 - Data Size: 356\n",
      "torgo_xlsr_finetune_F04 - Data Size: 340\n",
      "torgo_xlsr_finetune_M01 - Data Size: 352\n",
      "torgo_xlsr_finetune_M02 - Data Size: 405\n",
      "torgo_xlsr_finetune_M03 - Data Size: 285\n",
      "torgo_xlsr_finetune_M04 - Data Size: 432\n",
      "torgo_xlsr_finetune_M05 - Data Size: 404\n",
      "\n",
      "torgo_xlsr_finetune_F01 - Data Size: 217\n",
      "torgo_xlsr_finetune_F03 - Data Size: 317\n",
      "torgo_xlsr_finetune_F04 - Data Size: 243\n",
      "torgo_xlsr_finetune_M01 - Data Size: 239\n",
      "torgo_xlsr_finetune_M02 - Data Size: 256\n",
      "torgo_xlsr_finetune_M03 - Data Size: 242\n",
      "torgo_xlsr_finetune_M04 - Data Size: 223\n",
      "torgo_xlsr_finetune_M05 - Data Size: 149\n"
     ]
    }
   ],
   "source": [
    "%run 01_preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb86a76-1b1e-471c-91c7-a534e0b97ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ebaeaacca644ab8e5e2baca13c0700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71d5000b-b5f2-42e1-b886-46679eebd235",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_current = dataframes_word_keep_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68605e94-a918-4e1e-b278-9a554f299cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torgo_xlsr_finetune_F01_keep_all - Data Size: 336\n",
      "torgo_xlsr_finetune_F03_keep_all - Data Size: 328\n",
      "torgo_xlsr_finetune_F04_keep_all - Data Size: 296\n",
      "torgo_xlsr_finetune_M01_keep_all - Data Size: 399\n",
      "torgo_xlsr_finetune_M02_keep_all - Data Size: 534\n",
      "torgo_xlsr_finetune_M03_keep_all - Data Size: 288\n",
      "torgo_xlsr_finetune_M04__keep_all - Data Size: 449\n",
      "torgo_xlsr_finetune_M05_keep_all - Data Size: 452\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATTERN=\"word_keep\"\n",
    "\n",
    "for df_name, df in dataframes_word_keep_all.items():\n",
    "    print(f\"{df_name} - Data Size: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8453d5a-f4dc-43cd-a207-92ae07b2a478",
   "metadata": {},
   "source": [
    "# some atypical speech pattern not usable in normal transducer only G2P EN have REPRENSENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b0a1362-c48a-4bcc-be3b-358e47d0eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p\n",
    "\n",
    "g2p = G2p()\n",
    "\n",
    "for df_name, df in data_frame_current.items():\n",
    "    df['predictions_phoneme'] = df['predictions'].apply(lambda x: \" \".join(g2p(x)))\n",
    "    df['references_phoneme'] = df['references'].apply(lambda x: \" \".join(g2p(x)))\n",
    "\n",
    "    df.drop(['predictions', 'references'], axis=1, inplace=True)\n",
    "# dataframes_word_keep_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed9cf1-478c-4c02-a0e7-302346a3ac3a",
   "metadata": {},
   "source": [
    "# Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6067eec4-baad-4b00-adcd-8ad42de9d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in data_frame_current.items():\n",
    "    # Remove rows where columns are the same\n",
    "    df = df[df['predictions_phoneme'] != df['references_phoneme']]\n",
    "    \n",
    "    # Remove rows where columns only differ by the number of spaces\n",
    "    df = df[df.apply(lambda row: row['predictions_phoneme'].strip() != row['references_phoneme'].strip(), axis=1)]\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "# dataframes_word_keep_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12759a16-e051-44f1-b988-7f15fe4d4227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ori_train_dataframes_word_keep_all = {}\n",
    "test_dataframes_word_keep_all = {}\n",
    "\n",
    "for df_name, df in dataframes_word_keep_all.items():\n",
    "    # print(df)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_df_name = f\"{df_name}\"\n",
    "    test_df_name = f\"{df_name}_test\"\n",
    "    \n",
    "    ori_train_dataframes_word_keep_all[train_df_name] = train_df\n",
    "    test_dataframes_word_keep_all[test_df_name] = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec613a88-34f8-4d3c-9e48-4687c6697c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataframes_word_keep_all = {}\n",
    "val_dataframes_word_keep_all = {} \n",
    "\n",
    "for df_name, df in ori_train_dataframes_word_keep_all.items():\n",
    "    df = df.drop_duplicates(subset=['predictions_phoneme', 'references_phoneme'], keep=False)\n",
    "    \n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_df_name = f\"{df_name}_train\"\n",
    "    val_df_name = f\"{df_name}_val\"\n",
    "    \n",
    "    train_dataframes_word_keep_all[train_df_name] = train_df\n",
    "    val_dataframes_word_keep_all[val_df_name] = val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c442ef85-2c03-4786-a5b8-7abbca036c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'torgo_xlsr_finetune_F01_keep_all_train':       predictions_phoneme references_phoneme\n",
       " 15509   HH AY1 T R IH0 NG    HH IH1 T IH0 NG\n",
       " 15236           S OY1 R T            S UW1 T\n",
       " 15747             B AH1 D            B EH1 D\n",
       " 15461            HH EY1 L           HH EH1 M\n",
       " 14964             W EH1 T            M IY1 T\n",
       " ...                   ...                ...\n",
       " 14905             V AE1 T            B AE1 T\n",
       " 15637             W IY1 V            L EH1 R\n",
       " 15671             T IH1 L            T EH1 L\n",
       " 15914             B ER1 D            B ER1 D\n",
       " 15812       F L IH1 K ER0      S L IH1 K ER0\n",
       " \n",
       " [255 rows x 2 columns],\n",
       " 'torgo_xlsr_finetune_F03_keep_all_train':       predictions_phoneme references_phoneme\n",
       " 15056         B R AE1 G T            B ER1 D\n",
       " 14547               S AO1              S AY1\n",
       " 14782            S IH1 NG      S EH1 V AH0 N\n",
       " 14978             R EY1 T              EY1 T\n",
       " 14476              SH ER1           SH EH1 R\n",
       " ...                   ...                ...\n",
       " 15186            HH EH1 R           HH IH1 L\n",
       " 5758                  AY1            L IH1 K\n",
       " 7884            B EH1 S T          N EH1 S T\n",
       " 14492               F AA1             DH AH0\n",
       " 15234        JH EH1 S IY0            G IY1 S\n",
       " \n",
       " [252 rows x 2 columns],\n",
       " 'torgo_xlsr_finetune_F04_keep_all_train':        predictions_phoneme references_phoneme\n",
       " 15174                N OW1              N AO1\n",
       " 14301                AH1 V                OW1\n",
       " 15655            F AA1 R M          B AA1 R N\n",
       " 15701              T IH1 P           CH IH1 P\n",
       " 15862  P AA0 M T EH1 R OW0     HH OW0 T EH1 L\n",
       " ...                    ...                ...\n",
       " 15770           HH AA1 R T         HH AA1 R T\n",
       " 14667              B AE1 T            F AE1 T\n",
       " 15803            S IH1 N T         S IH1 NG K\n",
       " 15683             N IH1 SH            N AY1 S\n",
       " 15288              B L OW1          F L AO1 R\n",
       " \n",
       " [227 rows x 2 columns],\n",
       " 'torgo_xlsr_finetune_M01_keep_all_train':          predictions_phoneme references_phoneme\n",
       " 14575                R UW1 D            R UW1 T\n",
       " 15455  W IY1   R IH1 T AH0 N      R IH1 T AH0 N\n",
       " 15831               SH IH1 P           CH IH1 P\n",
       " 15697              T R IH1 P            P IH1 T\n",
       " 14697              K Y UW1 K    K W AH0 B EH1 K\n",
       " ...                      ...                ...\n",
       " 15695                AO1 R M            W AH1 N\n",
       " 14572               CH AA1 R             JH AO1\n",
       " 4959                   EH1 T            M AY1 L\n",
       " 15150                B IY1 T            B IY1 T\n",
       " 3614               EY1 T IY0              EY1 T\n",
       " \n",
       " [300 rows x 2 columns],\n",
       " 'torgo_xlsr_finetune_M02_keep_all_train':       predictions_phoneme references_phoneme\n",
       " 14872               T UW1           HH IH1 M\n",
       " 7822              N IH1 T            N AA1 T\n",
       " 15717             M Y UW1            M IH1 L\n",
       " 15385           G AA1 N T             DH OW1\n",
       " 15635               AE1 T              EY1 T\n",
       " ...                   ...                ...\n",
       " 7807                IH0 N      K W IH1 K ER0\n",
       " 15115           JH AH1 JH     G AE1 JH AH0 T\n",
       " 15421             M IH1 R            N IH1 R\n",
       " 14794         F AA1 L IY0     P IH1 T IH0 NG\n",
       " 15003           D AA1 R N            N AY1 N\n",
       " \n",
       " [422 rows x 2 columns],\n",
       " 'torgo_xlsr_finetune_M03_keep_all_train':       predictions_phoneme references_phoneme\n",
       " 6607                AA1 G                OW1\n",
       " 15506             T IH1 P            P IH1 T\n",
       " 15132             T AH1 N          T AO1 R N\n",
       " 15905             R AE1 K            B AE1 K\n",
       " 15701             T IH1 P           CH IH1 P\n",
       " ...                   ...                ...\n",
       " 15798            T IH1 NG      B IH1 T AH0 N\n",
       " 8118                N AW1              N OW1\n",
       " 15504       R IH1 T AH0 N      M IH1 T AH0 N\n",
       " 15181           S W OY1 T            S UW1 T\n",
       " 15767             T IH1 K            Z IH1 P\n",
       " \n",
       " [225 rows x 2 columns],\n",
       " 'torgo_xlsr_finetune_M04__keep_all_train':       predictions_phoneme references_phoneme\n",
       " 15206           B R UW1 T            B UW1 T\n",
       " 14671           D R IY1 T          T R EY1 T\n",
       " 15127             B AA1 N          B AO1 R N\n",
       " 15459               S IY1          S IY1 D Z\n",
       " 14616             P IY1 D            F IY1 D\n",
       " ...                   ...                ...\n",
       " 14538               AE1 T            B AE1 T\n",
       " 15768         AE1 L F AH0        P AA1 P AH0\n",
       " 15174               N UW1              N AO1\n",
       " 14891           S T EH1 R            F EH1 R\n",
       " 14818               IH1 T          S P IH1 T\n",
       " \n",
       " [345 rows x 2 columns],\n",
       " 'torgo_xlsr_finetune_M05_keep_all_train':            predictions_phoneme     references_phoneme\n",
       " 15094                 TH R UW1                  T UW1\n",
       " 1335                     EY1 P                  EY1 T\n",
       " 15336                  W EH1 L                W IY1 R\n",
       " 15937                F AO1 R T              F AO1 R K\n",
       " 15133                 SH IH1 N               SH EH1 R\n",
       " ...                        ...                    ...\n",
       " 15005  EY1 N   F AO1 R   Y UW1  Y UW1 N AH0 F AO2 R M\n",
       " 14870                  D IH1 R              D R UW1 P\n",
       " 14944                 CH IH1 P               CH AA1 P\n",
       " 14906          AY1   S K AO1 R            AO1 S K ER0\n",
       " 16020                M AH1 S T              L EH1 F T\n",
       " \n",
       " [345 rows x 2 columns]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataframes_word_keep_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55e6fceb-3ee8-470d-9bd1-8cdeca8486c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torgo_xlsr_finetune_F01_keep_all_train - Train Set Size: 255\n",
      "torgo_xlsr_finetune_F03_keep_all_train - Train Set Size: 252\n",
      "torgo_xlsr_finetune_F04_keep_all_train - Train Set Size: 227\n",
      "torgo_xlsr_finetune_M01_keep_all_train - Train Set Size: 300\n",
      "torgo_xlsr_finetune_M02_keep_all_train - Train Set Size: 422\n",
      "torgo_xlsr_finetune_M03_keep_all_train - Train Set Size: 225\n",
      "torgo_xlsr_finetune_M04__keep_all_train - Train Set Size: 345\n",
      "torgo_xlsr_finetune_M05_keep_all_train - Train Set Size: 345\n",
      "torgo_xlsr_finetune_F01_keep_all_val - Val Set Size: 29\n",
      "torgo_xlsr_finetune_F03_keep_all_val - Val Set Size: 28\n",
      "torgo_xlsr_finetune_F04_keep_all_val - Val Set Size: 26\n",
      "torgo_xlsr_finetune_M01_keep_all_val - Val Set Size: 34\n",
      "torgo_xlsr_finetune_M02_keep_all_val - Val Set Size: 47\n",
      "torgo_xlsr_finetune_M03_keep_all_val - Val Set Size: 26\n",
      "torgo_xlsr_finetune_M04__keep_all_val - Val Set Size: 39\n",
      "torgo_xlsr_finetune_M05_keep_all_val - Val Set Size: 39\n",
      "torgo_xlsr_finetune_F01_keep_all_test - Test Set Size: 34\n",
      "torgo_xlsr_finetune_F03_keep_all_test - Test Set Size: 33\n",
      "torgo_xlsr_finetune_F04_keep_all_test - Test Set Size: 30\n",
      "torgo_xlsr_finetune_M01_keep_all_test - Test Set Size: 40\n",
      "torgo_xlsr_finetune_M02_keep_all_test - Test Set Size: 54\n",
      "torgo_xlsr_finetune_M03_keep_all_test - Test Set Size: 29\n",
      "torgo_xlsr_finetune_M04__keep_all_test - Test Set Size: 45\n",
      "torgo_xlsr_finetune_M05_keep_all_test - Test Set Size: 46\n"
     ]
    }
   ],
   "source": [
    "for df_name, train_df in train_dataframes_word_keep_all.items():\n",
    "    print(f\"{df_name} - Train Set Size: {len(train_df)}\")\n",
    "\n",
    "for df_name, test_df in val_dataframes_word_keep_all.items():\n",
    "    print(f\"{df_name} - Val Set Size: {len(test_df)}\")\n",
    "\n",
    "for df_name, test_df in test_dataframes_word_keep_all.items():\n",
    "    print(f\"{df_name} - Test Set Size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c36e802c-8817-47ba-afde-97ae7536e7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker: torgo_xlsr_finetune_F01_keep_all_test\n",
      "Word Error Rate (WER): 57.63%\n",
      "Character Error Rate (CER): 42.31%\n",
      "Speaker: torgo_xlsr_finetune_F03_keep_all_test\n",
      "Word Error Rate (WER): 50.83%\n",
      "Character Error Rate (CER): 36.64%\n",
      "Speaker: torgo_xlsr_finetune_F04_keep_all_test\n",
      "Word Error Rate (WER): 60.19%\n",
      "Character Error Rate (CER): 41.63%\n",
      "Speaker: torgo_xlsr_finetune_M01_keep_all_test\n",
      "Word Error Rate (WER): 68.99%\n",
      "Character Error Rate (CER): 57.64%\n",
      "Speaker: torgo_xlsr_finetune_M02_keep_all_test\n",
      "Word Error Rate (WER): 65.78%\n",
      "Character Error Rate (CER): 44.94%\n",
      "Speaker: torgo_xlsr_finetune_M03_keep_all_test\n",
      "Word Error Rate (WER): 72.22%\n",
      "Character Error Rate (CER): 47.69%\n",
      "Speaker: torgo_xlsr_finetune_M04__keep_all_test\n",
      "Word Error Rate (WER): 74.84%\n",
      "Character Error Rate (CER): 51.27%\n",
      "Speaker: torgo_xlsr_finetune_M05_keep_all_test\n",
      "Word Error Rate (WER): 64.78%\n",
      "Character Error Rate (CER): 49.09%\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer,cer\n",
    "# Extract reference and hypothesis sentences\n",
    "\n",
    "for df_name, df in test_dataframes_word_keep_all.items():\n",
    "    # Assuming your DataFrame has 'references_phoneme' and 'predictions_phoneme' columns\n",
    "    references = df['references_phoneme'].tolist()\n",
    "    hypotheses = df['predictions_phoneme'].tolist()\n",
    "    \n",
    "    # Calculate WER and CER for the current speaker\n",
    "    wer_score = wer(references, hypotheses)\n",
    "    cer_score = cer(references, hypotheses)\n",
    "    \n",
    "    print(f'Speaker: {df_name}')\n",
    "    print(f'Word Error Rate (WER): {wer_score:.2%}')\n",
    "    print(f'Character Error Rate (CER): {cer_score:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71dc79f-5978-4b60-86fb-4b076c3b4e74",
   "metadata": {},
   "source": [
    "# toCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9e076fb-df8c-4d87-b737-a60e487e7d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_folder = f'data/{MODEL_PATTERN}'\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for df_name, train_df in train_dataframes_word_keep_all.items():\n",
    "    train_df.to_csv(os.path.join(output_folder, f'{df_name}.csv'), index=False)\n",
    "\n",
    "for df_name, val_df in val_dataframes_word_keep_all.items():\n",
    "    val_df.to_csv(os.path.join(output_folder, f'{df_name}.csv'), index=False)\n",
    "\n",
    "for df_name, test_df in test_dataframes_word_keep_all.items():\n",
    "    test_df.to_csv(os.path.join(output_folder, f'{df_name}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f2dd6-f70f-4a72-a593-7e681776cee1",
   "metadata": {},
   "source": [
    "# to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7883d7e-4d00-4625-a72c-f6e35338e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_json(csv_path, json_output_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    json_data = df.to_json(orient='records')\n",
    "\n",
    "    with open(json_output_path, 'w') as json_file:\n",
    "        json_file.write(json_data)\n",
    "\n",
    "csv_folder = f'data/{MODEL_PATTERN}'\n",
    "json_folder = f'data/{MODEL_PATTERN}'\n",
    "\n",
    "os.makedirs(json_folder, exist_ok=True)\n",
    "\n",
    "for csv_file_name in os.listdir(csv_folder):\n",
    "    if csv_file_name.endswith('.csv'):\n",
    "        csv_file_path = os.path.join(csv_folder, csv_file_name)\n",
    "        json_file_name = csv_file_name.replace('.csv', '.json')\n",
    "        json_output_file_path = os.path.join(json_folder, json_file_name)\n",
    "\n",
    "        convert_csv_to_json(csv_file_path, json_output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fea598d-7be6-40ae-b7a3-54a0dc61d21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558762d908e747a2ad84c283ee025d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78136aadf7b41bfabec08a68d0dc2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d80de2cecbc42079439231aaabfccd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1380ac96982e44c09aeaf0d90a628107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a759ec2816d437d833ee494df4cdc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd457d4d9434a6f8b7b62ab12a94c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb67270d1ba49d3a0d6ca182e80f9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f681ad7e29ce4811b182de8ff77a528c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b6b340d49e49f68525a4f2553afed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ae9eeafa114fa7b3ab38d716609ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731b3d2ddc4f470dbf3732898dc9905f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36e373740394fd8aad6d8c4c50d786c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c71382d19cc4a269c4b2ae3017e5a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96da1d01dcbc4031b71cb6419b9e9f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d69a2aaeaa4af0bed7466ac24c1cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db207a2d68c46a2858e175af5a681fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a186df1674847bb8621b94fc1b881df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2cca6ace5a4ac2b50b8d9b3bbefe05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130b91f5ed18496bb59e3ae3506ea9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4ef29e5a734260b1a8c45d0254f26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6dc81e91dc4799bb204e348a1c9758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e9d8f6dfd0430d89d524c585f8fd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389d774b6db94066b4250456b10874e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0471a48c494e97b0fdb7bc33c99ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPEAKER F01 - Train Set Size: 233\n",
      "SPEAKER F01 - Valid Set Size: 26\n",
      "SPEAKER F01 - Test Set Size: 31\n",
      "SPEAKER F03 - Train Set Size: 267\n",
      "SPEAKER F03 - Valid Set Size: 30\n",
      "SPEAKER F03 - Test Set Size: 36\n",
      "SPEAKER F04 - Train Set Size: 254\n",
      "SPEAKER F04 - Valid Set Size: 29\n",
      "SPEAKER F04 - Test Set Size: 34\n",
      "SPEAKER M01 - Train Set Size: 271\n",
      "SPEAKER M01 - Valid Set Size: 31\n",
      "SPEAKER M01 - Test Set Size: 36\n",
      "SPEAKER M02 - Train Set Size: 317\n",
      "SPEAKER M02 - Valid Set Size: 36\n",
      "SPEAKER M02 - Test Set Size: 41\n",
      "SPEAKER M03 - Train Set Size: 213\n",
      "SPEAKER M03 - Valid Set Size: 24\n",
      "SPEAKER M03 - Test Set Size: 29\n",
      "SPEAKER M04 - Train Set Size: 336\n",
      "SPEAKER M04 - Valid Set Size: 38\n",
      "SPEAKER M04 - Test Set Size: 44\n",
      "SPEAKER M05 - Train Set Size: 313\n",
      "SPEAKER M05 - Valid Set Size: 35\n",
      "SPEAKER M05 - Test Set Size: 41\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "loaded_datasets = {}\n",
    "\n",
    "for speaker in SPEAKERS:\n",
    "    file_pattern = f'torgo_xlsr_finetune_{speaker}_{{split}}.json'\n",
    "\n",
    "    loaded_datasets[speaker] = load_dataset('json', data_files={\n",
    "        'train': os.path.join(f'data/{MODEL_PATTERN}', file_pattern.format(split='train')),\n",
    "        'valid': os.path.join(f'data/{MODEL_PATTERN}', file_pattern.format(split='val')),\n",
    "        'test': os.path.join(f'data/{MODEL_PATTERN}', file_pattern.format(split='test')),\n",
    "    })\n",
    "\n",
    "for speaker, dataset in loaded_datasets.items():\n",
    "    print(f\"SPEAKER {speaker} - Train Set Size: {len(dataset['train'])}\")\n",
    "    print(f\"SPEAKER {speaker} - Valid Set Size: {len(dataset['valid'])}\")\n",
    "    print(f\"SPEAKER {speaker} - Test Set Size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b05ff81-babf-45da-a4dd-89d36b509938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['predictions_phoneme', 'references_phoneme'],\n",
       "        num_rows: 233\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['predictions_phoneme', 'references_phoneme'],\n",
       "        num_rows: 26\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['predictions_phoneme', 'references_phoneme'],\n",
       "        num_rows: 31\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_datasets['F01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4707957-6037-4e8b-9aed-43f387116413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4d95677-9247-4ca4-a466-8123d3f4b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_for_speaker(speaker_id):\n",
    "    print(speaker_id)\n",
    "    # Apply preprocess_function to train_data and val_data\n",
    "    train_data=loaded_datasets[speaker_id]['train']\n",
    "    val_data=loaded_datasets[speaker_id]['valid']\n",
    "    train_data = train_data.map(preprocess_function, batched=True)\n",
    "    val_data = val_data.map(preprocess_function, batched=True)\n",
    "\n",
    "    # Prepare DataLoader for training and validation\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=16)\n",
    "\n",
    "    output_dir = f\"torgo_spell_correction_{MODEL_PATTERN}_{speaker_id}\"\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=1e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=40,\n",
    "        predict_with_generate=True,\n",
    "        push_to_hub=True,\n",
    "        logging_steps=100\n",
    "    )\n",
    "\n",
    "    print(\"Train Dataset:\", train_data)\n",
    "    print(\"Validation Dataset:\", val_data)\n",
    "    print(\"Tokenizer:\", tokenizer)\n",
    "    print(\"Training Arguments:\", training_args)\n",
    "\n",
    "    model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # The Seq2SeqTrainer is created with the defined model, training arguments, datasets, tokenizer, and data_collator\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95373961-8533-4316-b5a3-2154c1df7fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfbdcff0a2343db8359f17a6ccab4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/233 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b927d8098da4bc9950d13b48d0c71be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: Dataset({\n",
      "    features: ['predictions_phoneme', 'references_phoneme', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 233\n",
      "})\n",
      "Validation Dataset: Dataset({\n",
      "    features: ['predictions_phoneme', 'references_phoneme', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 26\n",
      "})\n",
      "Tokenizer: BartTokenizerFast(name_or_path='facebook/bart-base', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True)\n",
      "Training Arguments: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=torgo_spell_correction_word_no_keep_F01/runs/Feb13_17-28-54_c2179,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=40,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=torgo_spell_correction_word_no_keep_F01,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=torgo_spell_correction_word_no_keep_F01,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 27:13, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.390423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.763874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.027802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.392777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.184684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.963527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.183900</td>\n",
       "      <td>1.834514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.183900</td>\n",
       "      <td>1.739363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.183900</td>\n",
       "      <td>1.609965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.183900</td>\n",
       "      <td>1.532629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5.183900</td>\n",
       "      <td>1.450243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.183900</td>\n",
       "      <td>1.381675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>5.183900</td>\n",
       "      <td>1.306632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.316600</td>\n",
       "      <td>1.245043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.316600</td>\n",
       "      <td>1.177319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.316600</td>\n",
       "      <td>1.115207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.316600</td>\n",
       "      <td>1.077142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.316600</td>\n",
       "      <td>1.030463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.316600</td>\n",
       "      <td>1.005150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.369100</td>\n",
       "      <td>0.958506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.369100</td>\n",
       "      <td>0.943867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.369100</td>\n",
       "      <td>0.907045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.369100</td>\n",
       "      <td>0.892933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.369100</td>\n",
       "      <td>0.879509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.369100</td>\n",
       "      <td>0.863923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.369100</td>\n",
       "      <td>0.863702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.845734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.825974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.824927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.825783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.823909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.823164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.825411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.670300</td>\n",
       "      <td>0.822519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.670300</td>\n",
       "      <td>0.812145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.670300</td>\n",
       "      <td>0.809088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.670300</td>\n",
       "      <td>0.802422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.670300</td>\n",
       "      <td>0.799900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.670300</td>\n",
       "      <td>0.798818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.606800</td>\n",
       "      <td>0.799206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d46bbc93b84d319436f194b6d9f414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d94ea9f4804cc9a4296f94cdd09d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: Dataset({\n",
      "    features: ['predictions_phoneme', 'references_phoneme', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 267\n",
      "})\n",
      "Validation Dataset: Dataset({\n",
      "    features: ['predictions_phoneme', 'references_phoneme', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 30\n",
      "})\n",
      "Tokenizer: BartTokenizerFast(name_or_path='facebook/bart-base', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True)\n",
      "Training Arguments: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=torgo_spell_correction_word_no_keep_F03/runs/Feb13_17-56-15_c2179,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=40,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=torgo_spell_correction_word_no_keep_F03,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=torgo_spell_correction_word_no_keep_F03,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 80/680 05:32 < 42:39, 0.23 it/s, Epoch 4.65/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.641427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.393003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.776828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.973657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model_for_speaker('F01')\n",
    "train_model_for_speaker('F03')\n",
    "train_model_for_speaker('F04')\n",
    "train_model_for_speaker('M01')\n",
    "train_model_for_speaker('M02')\n",
    "train_model_for_speaker('M03')\n",
    "train_model_for_speaker('M04')\n",
    "train_model_for_speaker('M05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b5d54-036d-43e5-a84a-461153e86af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "SPEAKERS = ['F01', 'F03', 'F04', 'M01', 'M02', 'M03', 'M04', 'M05']\n",
    "models = {}\n",
    "\n",
    "for speaker in SPEAKERS:\n",
    "    model_name = f\"matrixcc/torgo_spell_correction_{MODEL_PATTERN}_{speaker_id}\"\n",
    "    models[speaker] = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f87b87-3ead-4296-82e2-04c6b0bf2ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for speaker, model in models.items():\n",
    "    models[speaker] = model.to(device)\n",
    "    models[speaker].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3b362-dd10-4e1d-a443-d0e63c59af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_speaker(model, tokenizer, speaker_id):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    test_dataset = loaded_datasets[speaker_id]['test']\n",
    "    test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "    model.eval()\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"{example['predictions_phoneme']}\"\n",
    "        with torch.no_grad():\n",
    "            input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "            outputs = model.generate(input_ids=input_ids, max_length=max_length)\n",
    "\n",
    "        predicted_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        references.append(example['references_phoneme'])\n",
    "        predictions.append(predicted_sentence)\n",
    "\n",
    "    # Verify that the number of predictions and references are the same\n",
    "    if len(predictions) == len(references):\n",
    "        print(\"Number of predictions and references are the same.\")\n",
    "    else:\n",
    "        print(\"Mismatch in the number of predictions and references.\")\n",
    "\n",
    "    # Print the number of predictions and references\n",
    "    print(\"Number of predictions:\", len(predictions))\n",
    "    print(\"Number of references:\", len(references))\n",
    "    # print the length of the dataset\n",
    "    print(\"Number of rows in dataset:\", len(test_dataset))\n",
    "\n",
    "    # Assuming 'predictions' and 'references' are your sequences\n",
    "    # Calculate Word Error Rate (WER)\n",
    "    wer_value = wer(predictions, references)\n",
    "    wer_percentage = wer_value * 100\n",
    "    print(f\"WER for {speaker_id}: {wer_percentage:.2f}%\")\n",
    "\n",
    "    # Calculate Character Error Rate (CER)\n",
    "    cer_value = cer(predictions, references)\n",
    "    cer_percentage = cer_value * 100\n",
    "    print(f\"CER for {speaker_id}: {cer_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dadfaf-adb3-4d02-87e6-cae7dcba3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker_id, model in models.items():\n",
    "    evaluate_speaker(model, tokenizer, speaker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8409a6-3345-466d-b375-c5d767398d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edeedcb-3d52-4ff9-b87a-962d9d33d008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
