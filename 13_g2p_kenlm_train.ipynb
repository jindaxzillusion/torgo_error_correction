{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914ca930-3fb1-458e-b55e-1ae4fd93127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 10_kenlm_common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27dfba4b-1a68-4559-bcf7-950a8e892d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_dataset = \"dataset_phonemes.txt\"\n",
    "output_directory = \"output_directory/\"\n",
    "output_model_base = \"output_model.klm\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Ngram size options\n",
    "order_trigram = 3\n",
    "order_unigram = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e73e5-f66c-40ac-aa13-074e227de0c4",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66226b3-3c5c-478b-b7a7-a382f8a05a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_inference_on_cluster/dataset_phonemes.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 635344 types 42\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:504 2:37367230464 3:70063554560\n",
      "Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 42 D1=0.5 D2=1 D3+=1.5\n",
      "2 1280 D1=0.420074 D2=0.998284 D3+=1.12999\n",
      "3 16435 D1=0.483725 D2=1.009 D3+=1.52672\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 319 assuming -p 1.5\n",
      "probing 327 assuming -r models -p 1.5\n",
      "trie     88 without quantization\n",
      "trie     37 assuming -q 8 -b 8 quantization \n",
      "trie     87 assuming -a 22 array pointer compression\n",
      "trie     37 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:504 2:20480 3:328700\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:504 2:20480 3:328700\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:105493676 kB\tVmRSS:3904 kB\tRSSMax:24216720 kB\tuser:2.95713\tsys:10.9748\tCPU:13.9319\treal:13.9815\n",
      "/bin/bash: line 1: 16155 Done                    kenlm/build/bin/lmplz -o 3 --text dataset_phonemes.txt --arpa output_directory/output_model.klm_trigram.arpa --discount_fallback --skip_symbols\n",
      "     16156 Bus error               | kenlm/build/bin/build_binary -T /dev/stdin output_directory/output_model.klm_trigram.arpa\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz -o 3    --text dataset_phonemes.txt     --arpa output_directory/output_model.klm_trigram.arpa     --discount_fallback --skip_symbols|     kenlm/build/bin/build_binary     -T /dev/stdin output_directory/output_model.klm_trigram.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d535394e-8b6d-4a0d-ba09-5a195fbd0f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kenlm/build/bin/build_binary: error while loading shared libraries: libboost_program_options-mt.so.1.53.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/build_binary\\\n",
    "    output_directory/output_model.klm_trigram.arpa \\\n",
    "    output_directory/output_model.klm_trigram.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d05e0b-b5f4-4624-a685-d99b507e0cad",
   "metadata": {},
   "source": [
    "# compress model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f1ac4d-81e7-4f47-bd40-eb18540eff48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading output_directory/output_model.klm_trigram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/build_binary\\\n",
    "    output_directory/output_model.klm_trigram.arpa \\\n",
    "    output_directory/output_model.klm_trigram.klm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4202fad-c53f-4b09-827d-5823e571feb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'2=1280', '3=16435', '1=42'}\n",
      "Vocabulary: {'2=1280', '3=16435', '1=42'}\n"
     ]
    }
   ],
   "source": [
    "arpa_path = \"output_directory/output_model.klm_trigram.arpa\"\n",
    "vocabulary = get_vocabulary_from_arpa(arpa_path)\n",
    "# Print or use the vocabulary as needed\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "# Tokens to add\n",
    "new_tokens = [\"<pad>\", \"<sil>\", \"<spn>\"]\n",
    "# Tokens to exclude\n",
    "tokens_to_exclude = [\"<s>\", \"</s>\"]\n",
    "# Add tokens to the ARPA file excluding the specified tokens\n",
    "add_tokens_to_arpa(arpa_path, new_tokens, tokens_to_exclude)\n",
    "vocabulary = get_vocabulary_from_arpa(arpa_path)\n",
    "\n",
    "# Print or use the vocabulary as needed\n",
    "print(\"Vocabulary:\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279e63e-51fe-407d-82e7-1cb213e28fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
